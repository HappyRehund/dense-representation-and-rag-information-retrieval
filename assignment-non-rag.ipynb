{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03671fe6",
   "metadata": {},
   "source": [
    "# Sport Articles - Semantic Search\n",
    "\n",
    "## Objectives:\n",
    "1. Convert raw text into dense vector representations\n",
    "2. Build and query a vector index (FAISS)\n",
    "3. Evaluate semantic similarity between queries and documents\n",
    "\n",
    "## Pipeline:\n",
    "```\n",
    "Text â†’ Sliding Window Chunking â†’ Encoder â†’ Vectors â†’ FAISS Index â†’ Query â†’ Ranked Results\n",
    "```\n",
    "\n",
    "## Key Features:\n",
    "- **Sliding Window Chunking**: Split articles into overlapping chunks (500 chars, 100 overlap)\n",
    "- **Multilingual E5**: Dense encoder supporting Indonesian language\n",
    "- **FAISS IndexFlatIP**: Exact cosine similarity search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59f4fee",
   "metadata": {},
   "source": [
    "## Step 1: Load and Prepare Data\n",
    "\n",
    "Load the cleaned articles from `detik_sport_articles_cleaned.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bfdaf33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 397 articles\n",
      "\n",
      "Columns: ['url', 'title', 'date', 'author', 'content']\n",
      "âœ… All required columns present\n",
      "Removed 1 articles with very short content\n",
      "Final dataset: 396 articles\n",
      "\n",
      "Sample article:\n",
      "Title: Deco: Rashford Menderita di MU\n",
      "Content preview: Jakarta - Performa Marcus Rashford membaik di Barcelona . Direktur Barcelona Deco mengungkap penyebab Rashford kesulitan di Manchester United . Rashford dipinjamkan MU ke Barcelona pada musim panas lalu. Penyerang Inggris itu dilepas usai tidak masuk ke rencana Ruben Amorim. Ini jadi kali kedua seca...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load cleaned articles\n",
    "DATA_PATH = \"scraping_result/detik_sport_articles_cleaned.json\"\n",
    "\n",
    "with open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    articles = json.load(f)\n",
    "\n",
    "# Convert to DataFrame for easier manipulation\n",
    "df = pd.DataFrame(articles)\n",
    "print(f\"Loaded {len(df)} articles\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "\n",
    "# Validate required columns\n",
    "required_columns = ['title', 'content', 'date', 'url']\n",
    "missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "\n",
    "if missing_columns:\n",
    "    print(f\"âš ï¸ Missing columns: {missing_columns}\")\n",
    "    # Create placeholder columns if missing\n",
    "    for col in missing_columns:\n",
    "        if col == 'title':\n",
    "            df['title'] = df['content'].str[:50] + \"...\"\n",
    "        elif col == 'date':\n",
    "            df['date'] = \"Unknown\"\n",
    "        elif col == 'url':\n",
    "            df['url'] = \"\"\n",
    "else:\n",
    "    print(\"âœ… All required columns present\")\n",
    "\n",
    "# Remove rows with empty or very short content\n",
    "initial_count = len(df)\n",
    "df = df[df['content'].str.len() > 50].reset_index(drop=True)\n",
    "print(f\"Removed {initial_count - len(df)} articles with very short content\")\n",
    "print(f\"Final dataset: {len(df)} articles\")\n",
    "\n",
    "print(f\"\\nSample article:\")\n",
    "print(f\"Title: {df.iloc[0]['title']}\")\n",
    "print(f\"Content preview: {df.iloc[0]['content'][:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cfeb85",
   "metadata": {},
   "source": [
    "## Step 1b: Sliding Window Chunking\n",
    "\n",
    "Split long articles into overlapping chunks to:\n",
    "- **Preserve detailed context** within each chunk\n",
    "- **Maintain continuity** via overlap between consecutive chunks\n",
    "- **Improve retrieval precision** for specific facts\n",
    "\n",
    "**Parameters:**\n",
    "- `chunk_size`: Maximum characters per chunk (default: 500)\n",
    "- `overlap`: Characters shared between consecutive chunks (default: 100)\n",
    "\n",
    "```\n",
    "Article: [===========================================]\n",
    "Chunk 1: [========]\n",
    "Chunk 2:      [========]  â† overlaps with chunk 1\n",
    "Chunk 3:           [========]  â† overlaps with chunk 2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0e3d432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Chunking Parameters:\n",
      "   - Chunk size: 500 chars\n",
      "   - Overlap: 100 chars\n",
      "   - Effective step: 400 chars\n",
      "\n",
      "ğŸ“Š Chunking Results:\n",
      "   - Original articles: 396\n",
      "   - Total chunks: 2325\n",
      "   - Average chunks per article: 5.9\n",
      "\n",
      "ğŸ“ˆ Chunk Distribution:\n",
      "   - Min chunks: 2\n",
      "   - Max chunks: 22\n",
      "   - Articles with 1 chunk: 0\n",
      "   - Articles with 2+ chunks: 396\n"
     ]
    }
   ],
   "source": [
    "def sliding_window_chunking(\n",
    "    text: str,\n",
    "    chunk_size: int = 500,\n",
    "    overlap: int = 100\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks using sliding window.\n",
    "\n",
    "    Args:\n",
    "        text: Input text to chunk\n",
    "        chunk_size: Maximum characters per chunk\n",
    "        overlap: Characters shared between consecutive chunks\n",
    "\n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    if len(text) <= chunk_size:\n",
    "        return [text]\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "\n",
    "        # Try to break at sentence boundary (. ! ?)\n",
    "        if end < len(text):\n",
    "            # Look for last sentence boundary in the chunk\n",
    "            last_period = max(\n",
    "                chunk.rfind('. '),\n",
    "                chunk.rfind('! '),\n",
    "                chunk.rfind('? ')\n",
    "            )\n",
    "            if last_period > chunk_size // 2:  # Only if it's past halfway\n",
    "                chunk = chunk[:last_period + 1]\n",
    "                end = start + last_period + 1\n",
    "\n",
    "        chunks.append(chunk.strip())\n",
    "\n",
    "        # Move window forward (chunk_size - overlap)\n",
    "        start = end - overlap\n",
    "\n",
    "        # Avoid tiny final chunks\n",
    "        if len(text) - start < overlap:\n",
    "            break\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def chunk_articles(\n",
    "    df: pd.DataFrame,\n",
    "    chunk_size: int = 500,\n",
    "    overlap: int = 100\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply sliding window chunking to all articles.\n",
    "\n",
    "    Returns DataFrame with columns:\n",
    "    - article_id: Original article index\n",
    "    - chunk_id: Chunk index within article\n",
    "    - title: Article title (same for all chunks)\n",
    "    - content: Chunk content\n",
    "    - date, url: Original metadata\n",
    "    \"\"\"\n",
    "    chunks_data = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        full_text = row['content']\n",
    "        chunks = sliding_window_chunking(full_text, chunk_size, overlap)\n",
    "\n",
    "        for chunk_idx, chunk_text in enumerate(chunks):\n",
    "            chunks_data.append({\n",
    "                'article_id': idx,\n",
    "                'chunk_id': chunk_idx,\n",
    "                'title': row['title'],\n",
    "                'content': chunk_text,\n",
    "                'date': row['date'],\n",
    "                'url': row['url'],\n",
    "                'total_chunks': len(chunks)\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(chunks_data)\n",
    "\n",
    "\n",
    "# Apply chunking with parameters\n",
    "CHUNK_SIZE = 500  # characters per chunk\n",
    "OVERLAP = 100     # overlap between chunks\n",
    "\n",
    "print(f\"ğŸ“ Chunking Parameters:\")\n",
    "print(f\"   - Chunk size: {CHUNK_SIZE} chars\")\n",
    "print(f\"   - Overlap: {OVERLAP} chars\")\n",
    "print(f\"   - Effective step: {CHUNK_SIZE - OVERLAP} chars\")\n",
    "print()\n",
    "\n",
    "# Create chunked dataset\n",
    "df_chunks = chunk_articles(df, chunk_size=CHUNK_SIZE, overlap=OVERLAP)\n",
    "\n",
    "print(f\"ğŸ“Š Chunking Results:\")\n",
    "print(f\"   - Original articles: {len(df)}\")\n",
    "print(f\"   - Total chunks: {len(df_chunks)}\")\n",
    "print(f\"   - Average chunks per article: {len(df_chunks) / len(df):.1f}\")\n",
    "print()\n",
    "\n",
    "# Show chunk distribution\n",
    "chunk_counts = df_chunks.groupby('article_id')['chunk_id'].count()\n",
    "print(f\"ğŸ“ˆ Chunk Distribution:\")\n",
    "print(f\"   - Min chunks: {chunk_counts.min()}\")\n",
    "print(f\"   - Max chunks: {chunk_counts.max()}\")\n",
    "print(f\"   - Articles with 1 chunk: {(chunk_counts == 1).sum()}\")\n",
    "print(f\"   - Articles with 2+ chunks: {(chunk_counts >= 2).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9503c20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“° Sample Article: Deco: Rashford Menderita di MU\n",
      "   Total length: 1250 chars\n",
      "================================================================================\n",
      "\n",
      "ğŸ”ª Split into 4 chunks:\n",
      "\n",
      "â”â”â” Chunk 1/4 (447 chars) â”â”â”\n",
      "Jakarta - Performa Marcus Rashford membaik di Barcelona . Direktur Barcelona Deco mengungkap penyebab Rashford kesulitan di Manchester United . Rashford dipinjamkan MU ke Barcelona pada musim panas la...\n",
      "\n",
      "â”â”â” Chunk 2/4 (453 chars) â”â”â”\n",
      "umnya, pesepakbola berusia 28 tahun itu dipinjamkna ke Aston Villa pada paruh kedua musim 2024/2025. Bersama Barcelona, Rashford perlahan menemukan kembali performanya. Sejauh ini, ia sudah mencetak e...\n",
      "\n",
      "â”â”â” Chunk 3/4 (383 chars) â”â”â”\n",
      "ani. \"Rashford bahagia bersama kami. Marcus adalah pemain luar biasa,\" ujar Deco kepada Times Sport. \"Dia juga sedikit menderita dengan perubahan generasi di United. Itu tidak mudah. Ketika Anda adala...\n",
      "\n",
      "â”â”â” Chunk 4/4 (266 chars) â”â”â”\n",
      "lima tahun terakhir, mereka kesulitan membangun kembali tim, agar lebih kuat lagi. Dia ada di sana. Jadi tidak mudah bagi seorang pemain yang dapat banyak tuntutan.\" \"Ketika Anda adalah pemain penting...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualize chunking on a sample article\n",
    "sample_idx = df[df['content'].str.len() > 1000].index[0]  # Pick a long article\n",
    "sample_article = df.iloc[sample_idx]\n",
    "\n",
    "print(f\"ğŸ“° Sample Article: {sample_article['title']}\")\n",
    "print(f\"   Total length: {len(sample_article['content'])} chars\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "sample_chunks = df_chunks[df_chunks['article_id'] == sample_idx]\n",
    "print(f\"\\nğŸ”ª Split into {len(sample_chunks)} chunks:\\n\")\n",
    "\n",
    "for _, chunk_row in sample_chunks.iterrows():\n",
    "    chunk_num = chunk_row['chunk_id'] + 1\n",
    "    content = chunk_row['content']\n",
    "    print(f\"â”â”â” Chunk {chunk_num}/{chunk_row['total_chunks']} ({len(content)} chars) â”â”â”\")\n",
    "    print(content[:200] + \"...\" if len(content) > 200 else content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bb5bfe",
   "metadata": {},
   "source": [
    "## Step 2: Dense Encoding with Multilingual E5\n",
    "\n",
    "Using `intfloat/multilingual-e5-base` because:\n",
    "- âœ… Supports 100+ languages including **Indonesian**\n",
    "- âœ… State-of-the-art performance for multilingual semantic search\n",
    "- âœ… Good balance between accuracy and speed\n",
    "\n",
    "**Note:** E5 models require a prefix for the input:\n",
    "- For documents/passages: `\"passage: \"` + text\n",
    "- For queries: `\"query: \"` + text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afa42640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: intfloat/multilingual-e5-base\n",
      "Model loaded successfully!\n",
      "Embedding dimension: 768\n",
      "Model loaded successfully!\n",
      "Embedding dimension: 768\n"
     ]
    }
   ],
   "source": [
    "# Disable tqdm widget to avoid ipywidget rendering issues in VS Code\n",
    "os.environ[\"TQDM_DISABLE\"] = \"0\"  # Keep tqdm but use text mode\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import logging\n",
    "\n",
    "# Suppress unnecessary warnings\n",
    "logging.getLogger(\"sentence_transformers\").setLevel(logging.WARNING)\n",
    "\n",
    "# Load the multilingual E5 model\n",
    "MODEL_NAME = \"intfloat/multilingual-e5-base\"\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "\n",
    "model = SentenceTransformer(MODEL_NAME, device=\"cpu\")  # Explicitly set device\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Embedding dimension: {model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c007877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 2325 chunks for encoding\n",
      "(from 396 original articles)\n"
     ]
    }
   ],
   "source": [
    "# Prepare CHUNKS for encoding (not full documents)\n",
    "# E5 models require \"passage: \" prefix for documents\n",
    "\n",
    "def prepare_chunks_for_encoding(df_chunks: pd.DataFrame) -> list[str]:\n",
    "    \"\"\"\n",
    "    Prepare chunks for E5 encoding.\n",
    "    Combines title and chunk content with the required prefix.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    for _, row in df_chunks.iterrows():\n",
    "        # Combine title and chunk content for richer representation\n",
    "        text = f\"{row['title']}. {row['content']}\"\n",
    "        # Add E5 passage prefix\n",
    "        documents.append(f\"passage: {text}\")\n",
    "    return documents\n",
    "\n",
    "# Prepare all chunks\n",
    "documents = prepare_chunks_for_encoding(df_chunks)\n",
    "print(f\"Prepared {len(documents)} chunks for encoding\")\n",
    "print(f\"(from {len(df)} original articles)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a8395f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding documents... (this may take a few minutes)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb76ddb936bc42f4b5dcd91afa83f408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoding complete!\n",
      "Embeddings shape: (2325, 768)\n",
      "Each document is represented by a 768-dimensional vector\n"
     ]
    }
   ],
   "source": [
    "# Encode all documents into dense vectors\n",
    "print(\"Encoding documents... (this may take a few minutes)\")\n",
    "\n",
    "# Encode with text-based progress bar (not widget)\n",
    "embeddings = model.encode(\n",
    "    documents,\n",
    "    batch_size=32,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True  # Normalize for cosine similarity\n",
    ")\n",
    "\n",
    "print(f\"\\nEncoding complete!\")\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"Each document is represented by a {embeddings.shape[1]}-dimensional vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f6a63e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Embeddings saved to embedding/article_embeddings.npy\n",
      "   File size: 6.81 MB\n",
      "âœ… Chunk metadata saved to embedding/chunks_metadata.json\n",
      "   Total chunks: 2325\n"
     ]
    }
   ],
   "source": [
    "# Save embeddings and chunk metadata for later use\n",
    "EMBEDDINGS_PATH = \"embedding/article_embeddings.npy\"\n",
    "CHUNKS_PATH = \"embedding/chunks_metadata.json\"\n",
    "\n",
    "# Save embeddings\n",
    "np.save(EMBEDDINGS_PATH, embeddings)\n",
    "print(f\"âœ… Embeddings saved to {EMBEDDINGS_PATH}\")\n",
    "print(f\"   File size: {embeddings.nbytes / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Save chunk metadata (needed for retrieval)\n",
    "chunks_metadata = df_chunks.to_dict(orient='records')\n",
    "with open(CHUNKS_PATH, 'w', encoding='utf-8') as f:\n",
    "    json.dump(chunks_metadata, f, ensure_ascii=False, indent=2)\n",
    "print(f\"âœ… Chunk metadata saved to {CHUNKS_PATH}\")\n",
    "print(f\"   Total chunks: {len(chunks_metadata)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159ec4d4",
   "metadata": {},
   "source": [
    "## Step 3: Building FAISS Vector Index\n",
    "\n",
    "FAISS (Facebook AI Similarity Search) is a library for efficient similarity search and clustering of dense vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "975bec4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index created!\n",
      "Index type: Flat Inner Product (exact search)\n",
      "Dimension: 768\n",
      "Total vectors indexed: 2325\n",
      "\n",
      "ğŸ’¾ FAISS index saved to faiss/faiss_index.bin\n",
      "File size: 6.81 MB\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "# Get embedding dimension\n",
    "dimension = embeddings.shape[1]\n",
    "\n",
    "# Create FAISS index\n",
    "# Using IndexFlatIP for Inner Product (cosine similarity with normalized vectors)\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "\n",
    "# Add embeddings to the index\n",
    "index.add(embeddings.astype('float32'))\n",
    "\n",
    "print(f\"FAISS index created!\")\n",
    "print(f\"Index type: Flat Inner Product (exact search)\")\n",
    "print(f\"Dimension: {dimension}\")\n",
    "print(f\"Total vectors indexed: {index.ntotal}\")\n",
    "\n",
    "# Save FAISS index for later use\n",
    "INDEX_PATH = \"faiss/faiss_index.bin\"\n",
    "faiss.write_index(index, INDEX_PATH)\n",
    "print(f\"\\nğŸ’¾ FAISS index saved to {INDEX_PATH}\")\n",
    "print(f\"File size: {os.path.getsize(INDEX_PATH) / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0770bc6f",
   "metadata": {},
   "source": [
    "### Optional: Load Existing Embeddings and Index\n",
    "\n",
    "Run this cell instead of Step 2 & 3 if you already have saved embeddings and index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c380e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Found existing embeddings, chunks, and index. Loading...\n",
      "Loading model: intfloat/multilingual-e5-base\n",
      "âœ… Loaded embeddings: (2325, 768)\n",
      "âœ… Loaded FAISS index: 2325 vectors\n",
      "âœ… Loaded chunk metadata: 2325 chunks\n",
      "âœ… Model loaded for query encoding\n",
      "âœ… Loaded embeddings: (2325, 768)\n",
      "âœ… Loaded FAISS index: 2325 vectors\n",
      "âœ… Loaded chunk metadata: 2325 chunks\n",
      "âœ… Model loaded for query encoding\n"
     ]
    }
   ],
   "source": [
    "# Skip encoding if embeddings and index already exist\n",
    "# Run this cell INSTEAD of Step 1b, 2 & 3 encoding cells if you want to reload existing data\n",
    "\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "EMBEDDINGS_PATH = \"embedding/article_embeddings.npy\"\n",
    "CHUNKS_PATH = \"embedding/chunks_metadata.json\"\n",
    "INDEX_PATH = \"faiss/faiss_index.bin\"\n",
    "MODEL_NAME = \"intfloat/multilingual-e5-base\"\n",
    "\n",
    "if os.path.exists(EMBEDDINGS_PATH) and os.path.exists(INDEX_PATH) and os.path.exists(CHUNKS_PATH):\n",
    "    print(\"ğŸ“‚ Found existing embeddings, chunks, and index. Loading...\")\n",
    "\n",
    "    # Load embeddings\n",
    "    embeddings = np.load(EMBEDDINGS_PATH)\n",
    "\n",
    "    # Load FAISS index\n",
    "    index = faiss.read_index(INDEX_PATH)\n",
    "\n",
    "    # Load chunk metadata\n",
    "    with open(CHUNKS_PATH, 'r', encoding='utf-8') as f:\n",
    "        chunks_metadata = json.load(f)\n",
    "    df_chunks = pd.DataFrame(chunks_metadata)\n",
    "\n",
    "    # Load model for query encoding\n",
    "    print(f\"Loading model: {MODEL_NAME}\")\n",
    "    model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "    print(f\"âœ… Loaded embeddings: {embeddings.shape}\")\n",
    "    print(f\"âœ… Loaded FAISS index: {index.ntotal} vectors\")\n",
    "    print(f\"âœ… Loaded chunk metadata: {len(df_chunks)} chunks\")\n",
    "    print(f\"âœ… Model loaded for query encoding\")\n",
    "else:\n",
    "    missing = []\n",
    "    if not os.path.exists(EMBEDDINGS_PATH): missing.append(\"embeddings\")\n",
    "    if not os.path.exists(INDEX_PATH): missing.append(\"FAISS index\")\n",
    "    if not os.path.exists(CHUNKS_PATH): missing.append(\"chunk metadata\")\n",
    "    print(f\"âŒ Missing: {', '.join(missing)}. Please run Step 1b, 2 & 3 cells to encode documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d109b9e",
   "metadata": {},
   "source": [
    "## Step 4: Query and Retrieval\n",
    "\n",
    "Create a search function that:\n",
    "1. Encodes the query using the same model\n",
    "2. Uses FAISS to find top-k similar documents\n",
    "3. Returns titles and similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d767eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query: str, top_k: int = 5, deduplicate: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Search for semantically similar chunks.\n",
    "\n",
    "    Args:\n",
    "        query: Search query in Indonesian or English\n",
    "        top_k: Number of results to return\n",
    "        deduplicate: If True, return only best chunk per article\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with search results\n",
    "    \"\"\"\n",
    "    # E5 requires \"query: \" prefix for queries\n",
    "    query_with_prefix = f\"query: {query}\"\n",
    "\n",
    "    # Encode the query\n",
    "    query_embedding = model.encode(\n",
    "        [query_with_prefix],\n",
    "        normalize_embeddings=True,\n",
    "        convert_to_numpy=True\n",
    "    )\n",
    "\n",
    "    # Search in FAISS index (get more results if deduplicating)\n",
    "    search_k = top_k * 3 if deduplicate else top_k\n",
    "    scores, indices = index.search(query_embedding.astype('float32'), search_k)\n",
    "\n",
    "    # Build results from chunks\n",
    "    results = []\n",
    "    seen_articles = set()\n",
    "\n",
    "    for score, idx in zip(scores[0], indices[0]):\n",
    "        chunk = df_chunks.iloc[idx]\n",
    "        article_id = chunk['article_id']\n",
    "\n",
    "        # Skip if we already have a chunk from this article (deduplication)\n",
    "        if deduplicate and article_id in seen_articles:\n",
    "            continue\n",
    "        seen_articles.add(article_id)\n",
    "\n",
    "        results.append({\n",
    "            'rank': len(results) + 1,\n",
    "            'score': float(score),\n",
    "            'title': chunk['title'],\n",
    "            'date': chunk['date'],\n",
    "            'chunk_id': f\"{article_id}_{chunk['chunk_id']}\",\n",
    "            'chunk_num': f\"{chunk['chunk_id']+1}/{chunk['total_chunks']}\",\n",
    "            'content_preview': chunk['content'][:250] + \"...\" if len(chunk['content']) > 250 else chunk['content'],\n",
    "            'full_content': chunk['content'],\n",
    "            'url': chunk['url'],\n",
    "            'article_id': article_id\n",
    "        })\n",
    "\n",
    "        if len(results) >= top_k:\n",
    "            break\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def display_results(query: str, top_k: int = 5, deduplicate: bool = True):\n",
    "    \"\"\"Pretty print search results\"\"\"\n",
    "    print(f\"ğŸ” Query: \\\"{query}\\\"\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    results = search(query, top_k, deduplicate)\n",
    "\n",
    "    for _, row in results.iterrows():\n",
    "        print(f\"\\n#{row['rank']} | Score: {row['score']:.4f} | Chunk: {row['chunk_num']}\")\n",
    "        print(f\"ğŸ“° {row['title']}\")\n",
    "        print(f\"ğŸ“… {row['date']}\")\n",
    "        print(f\"ğŸ“ {row['content_preview']}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9702ff50",
   "metadata": {},
   "source": [
    "## Step 5: Qualitative Evaluation\n",
    "\n",
    "Test the semantic search with various queries in Indonesian to see if the results are semantically related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2ed6efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Query: \"Ronaldo Piala Dunia 2026\"\n",
      "================================================================================\n",
      "\n",
      "#1 | Score: 0.8770 | Chunk: 1/3\n",
      "ğŸ“° Ronaldo Lolos Sanksi, Bisa Main di Fase Grup Piala Dunia 2026\n",
      "ğŸ“… Rabu, 26 Nov 2025 07:00 WIB\n",
      "ğŸ“ Zurich - Timnas Portugal bisa bernapas lega. Sebab Cristiano Ronaldo lolos dari sanksi kartu merah dan bisa main di Piala Dunia 2026 . Sebelumnya, Ronaldo dikartumerah saat Portugal tumbang 0-2 di kandang Republik Irlandia, karena menyikut bek lawan ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "#2 | Score: 0.8729 | Chunk: 2/6\n",
      "ğŸ“° Netizen 'Ngamuk' Usai FIFA Ringankan Hukuman Ronaldo di Piala Dunia 2026\n",
      "ğŸ“… Rabu, 26 Nov 2025 10:08 WIB\n",
      "ğŸ“ obaan. Dari tiga laga, satu pertandingan sudah dilewatkan Ronaldo ketika Portugal menggilas Armenia. Dua laga berikutnya, yang harusnya dimainkan Portugal di Piala Dunia 2026, CR7 tetap bisa tampil. Sanksi dua laga berikutnya ditangguhkan FIFA untuk ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "#3 | Score: 0.8615 | Chunk: 3/3\n",
      "ğŸ“° Pot Drawing Piala Dunia 2026: Potensi Haaland Vs Messi atau Ronaldo\n",
      "ğŸ“… Rabu, 26 Nov 2025 14:00 WIB\n",
      "ğŸ“ -siap lihat duel Haaland vs Messi atau Ronaldo! Selain itu, ada potensi Portugal ketemu Maroko lagi. Maroko sempat bikin patah hati Portugal di Piala Dunia 2022, menyingkirkannya di perempatfinal. Undian pembagian grup Piala Dunia 2026 akan berlangsu...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "#4 | Score: 0.8454 | Chunk: 1/3\n",
      "ğŸ“° Lamine Yamal Tak Sabar Main di Piala Dunia 2026\n",
      "ğŸ“… Senin, 01 Des 2025 19:51 WIB\n",
      "ğŸ“ Jakarta - Lamine Yamal sudah tak sabar untuk berlaga di Piala Dunia 2026 . Pemain 18 tahun itu menegaskan pesta bola sejagad datang di saat yang tepat. Piala Dunia 2026 akan digelar di Amerika Serikat, Kanada, dan Meksiko. Ajang itu berlangsung 11 Ju...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "#5 | Score: 0.8443 | Chunk: 3/4\n",
      "ğŸ“° Ronaldo Pamer Gol Saltonya di Medsos, Minta Tanggapan Netizen\n",
      "ğŸ“… Senin, 24 Nov 2025 09:30 WIB\n",
      "ğŸ“ ab, Cristiano Ronaldo menulis captions yang meminta netizen memberi caption atas video gol saltonya. \"Caption terbaik bakal menang,\" tulis Ronaldo dengan emoji berpikir, semacam membuat sayembara ke netizen memberi tanggapannya. Best caption wins! Ã°ÂŸ...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test Query 1: Tentang pemain sepakbola tertentu\n",
    "results1 = display_results(\"Ronaldo Piala Dunia 2026\", top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb016fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Query: \"Klasemen Liga Inggris terbaru\"\n",
      "================================================================================\n",
      "\n",
      "#1 | Score: 0.8444 | Chunk: 3/5\n",
      "ğŸ“° Jadwal Liga Inggris Tengah Pekan Ini: Man City Main Nanti Malam\n",
      "ğŸ“… Selasa, 02 Des 2025 09:40 WIB\n",
      "ğŸ“ ujian dari Sunderland. The Reds akan menjamu Granit Xhaka dkk. di Anfield pada Kamis dini hari WIB. Liverpool baru saja memutus tren negatifnya dengan kemenangan 2-0 di markas West Ham United akhir pekan lalu. Menilik klasemen Liga Inggris, Liverpool...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "#2 | Score: 0.8398 | Chunk: 1/4\n",
      "ğŸ“° Pemain dengan Assist Terbanyak di Liga Inggris\n",
      "ğŸ“… Selasa, 02 Des 2025 14:40 WIB\n",
      "ğŸ“ Daftar Isi Daftar Pemain dengan Assist Terbanyak di Liga Inggris Jakarta - Liga Inggris, atau yang biasa dikenal sebagai Premier League, merupakan liga sepakbola yang amat populer di dunia. Terdapat 51 klub yang sudah berkompetisi di liga ini sejak d...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "#3 | Score: 0.8390 | Chunk: 4/5\n",
      "ğŸ“° Klasemen Liga Spanyol: Barca Tempel Real Madrid, Beda 1 Poin\n",
      "ğŸ“… Senin, 24 Nov 2025 09:00 WIB\n",
      "ğŸ“ Klasemen Liga Spanyol Pos Klub M M S K G K +/- POIN 1 Real Madrid 13 10 2 1 28 12 16 32 2 Barcelona 13 10 1 2 36 15 21 31 3 Villarreal 13 9 2 2 26 11 15 29 4 Atletico Madrid 13 8 4 1 25 11 14 28 5 Real Betis 13 5 6 2 20 14 6 21 6 Espanyol 12 5 3 4 15...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "#4 | Score: 0.8354 | Chunk: 5/6\n",
      "ğŸ“° Hasil dan Klasemen Liga Spanyol: Real Madrid Tertahan, Dikudeta Barcelona\n",
      "ğŸ“… Senin, 01 Des 2025 09:15 WIB\n",
      "ğŸ“ celona 14 11 1 2 39 16 23 34 2 Real Madrid 14 10 3 1 29 13 16 33 3 Villarreal 14 10 2 2 26 13 16 32 4 Atletico Madrid 14 9 4 1 27 11 16 31 5 Real Betis 14 6 6 2 22 14 8 24 6 Espanyol 14 7 3 4 18 16 2 24 7 Getafe 14 6 2 6 13 15 -2 20 8 Athletic Bilbao...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "#5 | Score: 0.8351 | Chunk: 1/3\n",
      "ğŸ“° Sir Beckham: Amorim Pelan-pelan Bawa MU Tampil Sip!\n",
      "ğŸ“… Senin, 01 Des 2025 20:08 WIB\n",
      "ğŸ“ Jakarta - Manchester United dinilai mulai membaik di bawah asuhan Ruben Amorim. Sir David Beckham yang mengungkap hal itu. Di klasemen Liga Inggris saat ini, MU ada di posisi ketujuh. The Red Devils mengumpulkan 21 poin, berjarak sembilan poin dari A...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test Query 2: Tentang liga tertentu\n",
    "results2 = display_results(\"Klasemen Liga Inggris terbaru\", top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98411195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Query: \"Pelatih baru Timnas Indonesia\"\n",
      "================================================================================\n",
      "\n",
      "#1 | Score: 0.8488 | Chunk: 1/2\n",
      "ğŸ“° Gabung Navbahor, Kapadze Dipastikan Tidak Latih Timnas Indonesia\n",
      "ğŸ“… Selasa, 02 Des 2025 00:00 WIB\n",
      "ğŸ“ Jakarta - Timur Kapadze dipastikan tidak akan menjadi pelatih Timnas Indonesia . Dia sudah menerima pinangan klub Navbahor Namangan. \"Navbahor hari ini secara resmi memperkenalkan Timur Kapadze sebagai pelatih kepala baru. Kedua pihak telah menandata...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "#2 | Score: 0.8412 | Chunk: 7/7\n",
      "ğŸ“° SEA Games 2025: Demi Quattrick Emas, Timnas Voli RI ke China\n",
      "ğŸ“… Minggu, 16 Nov 2025 12:50 WIB\n",
      "ğŸ“ fal Mauluddani Kristoforus Sina Jasen Natanael Kilanta Alfin Daniel Pratama Prasojo Fahreza Rakha Abhinaya Ofisial: Jeff Jiang , Erwin Rusni, Novie Efendi Lestari Mulyono, Setiawan Adi Praja Kusuma Nur Amanu, Aditya Pahlevi Manajer: Adang Ginanjar se...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "#3 | Score: 0.8325 | Chunk: 1/4\n",
      "ğŸ“° Pelatih Timnas Basket RI Kantongi Skuad Bayangan untuk SEA Games 2025\n",
      "ğŸ“… Sabtu, 29 Nov 2025 18:15 WIB\n",
      "ğŸ“ Jakarta - Pelatih Timnas basket Indonesia David Singleton sudah mengantongi skuad bayangan menjelang SEA Games Thailand yang bergulir bulan depan. Skuad final berisikan 12 pemain. Hasil itu diperoleh melalui hasil uji coba yang dilakukan Timnas melaw...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "#4 | Score: 0.8316 | Chunk: 2/5\n",
      "ğŸ“° TC Timnas Basket SEA Games: David Singleton Pulangkan 6 Pemain\n",
      "ğŸ“… Kamis, 02 Okt 2025 16:50 WIB\n",
      "ğŸ“ ni nyaris sebulan berjalan, Pelatih Timnas Putra Senior David Singleton melakukan perampingan skuad. Enam pemain dipulangkan ke klub masing-masing. \"Seperti yang sudah saya sampaikan sebelumnya, proses ini adalah tentang memilih yang terbaik dari yan...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "#5 | Score: 0.8289 | Chunk: 3/4\n",
      "ğŸ“° BTN Panggil 24 Pebasket Ikuti TC SEA Games 2025\n",
      "ğŸ“… Senin, 08 Sep 2025 18:30 WIB\n",
      "ğŸ“ Yudha Saputera, Widyanta Teja, Andakara Prastawa, Dame Diagne, Juan Laurent, hingga Julian Chalias. Mengenai regulasi pemain naturalisasi di SEA Games 2025, federasi memastikan belum berubah. \"Aturan tersebut menyebutkan hanya pemain yang memiliki pa...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test Query 3: Tentang Timnas Indonesia\n",
    "results3 = display_results(\"Pelatih baru Timnas Indonesia\", top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0980eb16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Query: \"Barcelona beli pemain baru\"\n",
      "================================================================================\n",
      "\n",
      "#1 | Score: 0.8231 | Chunk: 1/3\n",
      "ğŸ“° Hari Sempurna buat Barcelona: Main Lagi di Camp Nou, Menang, Clean Sheet\n",
      "ğŸ“… Minggu, 23 Nov 2025 11:00 WIB\n",
      "ğŸ“ Jakarta - Barcelona menandai comeback-nya ke Camp Nou dengan kemenangan telak atas Athletic Bilbao . Segalanya terasa sempurna untuk Blaugrana. Barcelona menjamu Bilbao di Camp Nou dalam lanjutan LaLiga , Sabtu malam WIB. Ini jadi pertandingan pertam...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "#2 | Score: 0.8210 | Chunk: 1/8\n",
      "ğŸ“° Barcelona Vs Bilbao: Blaugrana Menang 4-0, Naik ke Puncak Klasemen\n",
      "ğŸ“… Minggu, 23 Nov 2025 00:33 WIB\n",
      "ğŸ“ Jakarta - Barcelona menang 4-0 atas Athletic Bilbao dalam pertandingan di Camp Nou. Hasil ini membawa Barcelona menggeser Real Madrid dari puncak klasemen. Laga Barcelona vs Bilbao di pekan ke-13 LaLiga digelardi Camp Nou, Sabtu malam WIB. Blaugrana ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "#3 | Score: 0.8171 | Chunk: 1/4\n",
      "ğŸ“° Barcelona Vs Bilbao: Saatnya Barca Rebut Puncak Klasemen di Camp Nou\n",
      "ğŸ“… Sabtu, 22 Nov 2025 15:00 WIB\n",
      "ğŸ“ Barcelona - Barcelona kembali berlaga di Camp Nou usai nyaris tiga tahun. Kemenangan atas Athletic Bilbao akan melambungkan Barca ke puncak klasemen, setidaknya untuk sementara. Stadion Camp Nou resmi dibuka lagi setelah pemugaran, yang dimulai pada ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "#4 | Score: 0.8170 | Chunk: 1/4\n",
      "ğŸ“° Usia Hanya Sekadar Angka untuk Lewandowski\n",
      "ğŸ“… Minggu, 23 Nov 2025 19:30 WIB\n",
      "ğŸ“ Barcelona - Pelatih Barcelona , Hansi Flick , menegaskan bahwa usia bukan halangan untuk Robert Lewandowski terus tampil tajam. Ia masih sangat mengandalkan penyerang 37 tahun ini di lini depan. Barcelona meraih kemenangan 4-0 atas Bilbao pada pekan ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "#5 | Score: 0.8119 | Chunk: 1/4\n",
      "ğŸ“° Flick Senang Barca Menang, tapi Satu Hal Ini Wajib Diperbaiki\n",
      "ğŸ“… Minggu, 30 Nov 2025 06:30 WIB\n",
      "ğŸ“ Barcelona - Barcelona memuncaki klasemen LaLiga usai mengandaskan Alaves 3-1. Hansi Flick senang dengan hasil ini, sekaligus menegaskan finishing Barca perlu ditingkatkan. Pada pertandingan di Camp Nou, Sabtu malam WIB, Barca memulai dengan buruk. Al...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test Query 4: Tentang transfer pemain\n",
    "results4 = display_results(\"Barcelona beli pemain baru\", top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa126870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Query: \"juara dunia badminton\"\n",
      "================================================================================\n",
      "\n",
      "#1 | Score: 0.8295 | Chunk: 6/8\n",
      "ğŸ“° Atlet Muda RI Sukses Kuasai Podium Wondr by BNI International Challenge\n",
      "ğŸ“… Selasa, 25 Nov 2025 12:16 WIB\n",
      "ğŸ“ i konkret bagi lahirnya bintang-bintang baru yang siap menjaga dominasi Indonesia di panggung dunia. Panen gelar di Yogyakarta menjadi bukti bahwa masa depan bulutangkis Indonesia berada di tangan generasi muda yang telah terbina dengan baik dan siap...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "#2 | Score: 0.8231 | Chunk: 2/3\n",
      "ğŸ“° Perbati Jadi Anggota World Boxing\n",
      "ğŸ“… Senin, 24 Nov 2025 02:30 WIB\n",
      "ğŸ“ Kongres World Boxing,\" kata Ketua Umum Perbati, Ray Zulham Farras Nugraha, dalam keterangan persnya. Ketua Ketua Umum Komite Olimpiade Indonesia , Raja Sapta Oktohari, sempat hadir dalam kongres tersebut. Bersama Okto, Ray bertemu Gennady Golovkin se...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "#3 | Score: 0.8196 | Chunk: 1/6\n",
      "ğŸ“° Kisah Ralyya, Pebulutangkis Cilik Batam Bergelimang Prestasi\n",
      "ğŸ“… Kamis, 27 Nov 2025 02:30 WIB\n",
      "ğŸ“ Kota Batam - Bibit muda pebulutangkis datang dari Kepulauan Riau . Dia adalah Ralyya Van Rajab Siregar, atlet cilik Kota Batam dengan segudang prestasi. Podium pertama diraih Ralyya pada kejuaraan bulutangkis 7th JAVY Badminton Youth Cup di GOR Banda...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "#4 | Score: 0.8181 | Chunk: 1/7\n",
      "ğŸ“° Juara IBL 2025, Presiden Dewa United Banten Michael OW: Veni Vidi Vici\n",
      "ğŸ“… Selasa, 22 Jul 2025 20:12 WIB\n",
      "ğŸ“ Jakarta - Presiden Dewa United Banten, Michael Oliver Wellerz , sangat bangga atas keberhasilan klubnya mencetak sejarah IBL. Seperti diketahui, Dewa United Banten sukses mendapat gelar juara IBL 2025 sekaligus menjadi yang terbaik di kasta tertinggi...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "#5 | Score: 0.8171 | Chunk: 3/4\n",
      "ğŸ“° Dua Atlet Binaraga DKI Sabet Medali di Kejuaraan Dunia 2025\n",
      "ğŸ“… Senin, 17 Nov 2025 01:10 WIB\n",
      "ğŸ“ raih Emas,\" tegas Zainal dan Junaedi serta Ansori dalam siaran pers yang diterima wartawan, Minggu . Ketua PBFI DKI Jakarta Estepanus Tengkomengatakan tiga atlet yang mengikuti kejuaraan dunia di Batam adalah atlet pusat pelatihan daerah yang didukun...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test Query 5: Query dalam bahasa Inggris (test multilingual capability)\n",
    "results5 = display_results(\"juara dunia badminton\", top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514cd007",
   "metadata": {},
   "source": [
    "## Step 5b: Quantitative Evaluation\n",
    "\n",
    "Evaluate search quality with predefined test cases using **Precision@K** metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "048796b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_search_quality(test_queries: list[dict], top_k: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluate search quality with predefined test cases.\n",
    "\n",
    "    Args:\n",
    "        test_queries: List of dicts with 'query' and 'expected_keywords'\n",
    "        top_k: Number of results to evaluate\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with evaluation metrics\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for test in test_queries:\n",
    "        query = test['query']\n",
    "        expected_keywords = test['expected_keywords']\n",
    "\n",
    "        search_results = search(query, top_k=top_k, deduplicate=True)\n",
    "\n",
    "        # Calculate hits: how many results contain at least one expected keyword\n",
    "        hits = 0\n",
    "        for _, row in search_results.iterrows():\n",
    "            title_content = (row['title'] + \" \" + row['full_content']).lower()\n",
    "            if any(kw.lower() in title_content for kw in expected_keywords):\n",
    "                hits += 1\n",
    "\n",
    "        precision_at_k = hits / top_k\n",
    "\n",
    "        results.append({\n",
    "            'query': query,\n",
    "            'expected_keywords': ', '.join(expected_keywords[:3]) + '...',\n",
    "            f'hits@{top_k}': hits,\n",
    "            f'precision@{top_k}': precision_at_k,\n",
    "            'top_score': search_results.iloc[0]['score'] if len(search_results) > 0 else 0,\n",
    "            'avg_score': search_results['score'].mean() if len(search_results) > 0 else 0\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "953b32c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Quantitative Evaluation Results\n",
      "==========================================================================================\n",
      "                    query                         expected_keywords  hits@5  precision@5  top_score  avg_score\n",
      "  Ronaldo gol Piala Dunia                ronaldo, cristiano, cr7...       5          1.0   0.856364   0.845966\n",
      "    Liga Inggris klasemen premier league, liga inggris, klasemen...       5          1.0   0.843397   0.830476\n",
      " Timnas Indonesia pelatih             timnas, indonesia, pelatih...       5          1.0   0.842489   0.833422\n",
      "Barcelona transfer pemain             barcelona, barca, transfer...       5          1.0   0.828815   0.820697\n",
      "     MotoGP balapan juara                   motogp, motor, juara...       5          1.0   0.841939   0.838929\n",
      "      Liga Italia Serie A           serie a, liga italia, italia...       4          0.8   0.824424   0.807094\n",
      "Manchester United masalah                 manchester, united, mu...       5          1.0   0.841527   0.835348\n",
      "\n",
      "==========================================================================================\n",
      "ğŸ“ˆ Average Precision@5: 97.14%\n",
      "ğŸ“ˆ Average Top Score: 0.8399\n",
      "ğŸ“ˆ Average Score: 0.8303\n"
     ]
    }
   ],
   "source": [
    "# Define test cases with expected keywords\n",
    "test_queries = [\n",
    "    {\n",
    "        'query': 'Ronaldo gol Piala Dunia',\n",
    "        'expected_keywords': ['ronaldo', 'cristiano', 'cr7', 'gol', 'piala dunia']\n",
    "    },\n",
    "    {\n",
    "        'query': 'Liga Inggris klasemen',\n",
    "        'expected_keywords': ['premier league', 'liga inggris', 'klasemen', 'epl', 'inggris']\n",
    "    },\n",
    "    {\n",
    "        'query': 'Timnas Indonesia pelatih',\n",
    "        'expected_keywords': ['timnas', 'indonesia', 'pelatih', 'garuda', 'pssi']\n",
    "    },\n",
    "    {\n",
    "        'query': 'Barcelona transfer pemain',\n",
    "        'expected_keywords': ['barcelona', 'barca', 'transfer', 'beli', 'blaugrana']\n",
    "    },\n",
    "    {\n",
    "        'query': 'MotoGP balapan juara',\n",
    "        'expected_keywords': ['motogp', 'motor', 'juara', 'race', 'gp', 'balapan']\n",
    "    },\n",
    "    {\n",
    "        'query': 'Liga Italia Serie A',\n",
    "        'expected_keywords': ['serie a', 'liga italia', 'italia', 'inter', 'milan', 'juventus']\n",
    "    },\n",
    "    {\n",
    "        'query': 'Manchester United masalah',\n",
    "        'expected_keywords': ['manchester', 'united', 'mu', 'man utd', 'old trafford']\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run evaluation\n",
    "eval_results = evaluate_search_quality(test_queries, top_k=5)\n",
    "\n",
    "# Display results\n",
    "print(\"ğŸ“Š Quantitative Evaluation Results\")\n",
    "print(\"=\" * 90)\n",
    "print(eval_results.to_string(index=False))\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(f\"ğŸ“ˆ Average Precision@5: {eval_results['precision@5'].mean():.2%}\")\n",
    "print(f\"ğŸ“ˆ Average Top Score: {eval_results['top_score'].mean():.4f}\")\n",
    "print(f\"ğŸ“ˆ Average Score: {eval_results['avg_score'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89385cd",
   "metadata": {},
   "source": [
    "## Summary Statistics\n",
    "\n",
    "Overview of the semantic search system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c8592da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ˆ Semantic Search System Statistics\n",
      "==================================================\n",
      "ğŸ“š Original articles: 396\n",
      "ğŸ”ª Total chunks indexed: 2325\n",
      "ğŸ“Š Avg chunks per article: 5.9\n",
      "ğŸ“ Embedding dimension: 768\n",
      "ğŸ¤– Model: intfloat/multilingual-e5-base\n",
      "ğŸ” Index type: FAISS Flat Inner Product\n",
      "\n",
      "âš™ï¸ Chunking Parameters:\n",
      "   - Chunk size: 500 chars\n",
      "   - Overlap: 100 chars\n",
      "\n",
      "ğŸ’¾ Storage:\n",
      "   - Embeddings: 6.81 MB\n",
      "   - FAISS index: 6.81 MB\n",
      "   - Chunk metadata: 1703.7 KB\n",
      "\n",
      "ğŸ“Š Evaluation:\n",
      "   - Average Precision@5: 97.14%\n",
      "   - Best performing query: Ronaldo gol Piala Dunia\n",
      "   - Worst performing query: Liga Italia Serie A\n"
     ]
    }
   ],
   "source": [
    "INDEX_PATH = \"faiss/faiss_index.bin\"\n",
    "EMBEDDINGS_PATH = \"embedding/article_embeddings.npy\"\n",
    "CHUNKS_PATH = \"embedding/chunks_metadata.json\"\n",
    "\n",
    "print(\"ğŸ“ˆ Semantic Search System Statistics\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"ğŸ“š Original articles: {len(df)}\")\n",
    "print(f\"ğŸ”ª Total chunks indexed: {len(df_chunks)}\")\n",
    "print(f\"ğŸ“Š Avg chunks per article: {len(df_chunks) / len(df):.1f}\")\n",
    "print(f\"ğŸ“ Embedding dimension: {embeddings.shape[1]}\")\n",
    "print(f\"ğŸ¤– Model: intfloat/multilingual-e5-base\")\n",
    "print(f\"ğŸ” Index type: FAISS Flat Inner Product\")\n",
    "print(f\"\\nâš™ï¸ Chunking Parameters:\")\n",
    "print(f\"   - Chunk size: {CHUNK_SIZE} chars\")\n",
    "print(f\"   - Overlap: {OVERLAP} chars\")\n",
    "print(f\"\\nğŸ’¾ Storage:\")\n",
    "print(f\"   - Embeddings: {embeddings.nbytes / 1024 / 1024:.2f} MB\")\n",
    "if os.path.exists(INDEX_PATH):\n",
    "    print(f\"   - FAISS index: {os.path.getsize(INDEX_PATH) / 1024 / 1024:.2f} MB\")\n",
    "if os.path.exists(CHUNKS_PATH):\n",
    "    print(f\"   - Chunk metadata: {os.path.getsize(CHUNKS_PATH) / 1024:.1f} KB\")\n",
    "print(f\"\\nğŸ“Š Evaluation:\")\n",
    "print(f\"   - Average Precision@5: {eval_results['precision@5'].mean():.2%}\")\n",
    "print(f\"   - Best performing query: {eval_results.loc[eval_results['precision@5'].idxmax(), 'query']}\")\n",
    "print(f\"   - Worst performing query: {eval_results.loc[eval_results['precision@5'].idxmin(), 'query']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-sport-articles (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
